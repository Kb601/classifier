uncertain reasoning

uncertain reasoning

to view a paper, click on the open book image.  









combining symbolic and connectionist learning methods to refine
certainty-factor rule-bases
j. jeffrey mahoney

ph.d. thesis, department of computer sciences, university of texas at austin, may, 1996.


this research describes the system rapture, which is designed to
revise rule bases expressed in certainty-factor format.  recent
studies have shown that learning is facilitated when biased with
domain-specific expertise, and have also shown that many real-world
domains require some form of probabilistic or uncertain reasoning in
order to successfully represent target concepts. rapture was designed
to take advantage of both of these results. 

beginning with a set of certainty-factor rules, along with
accurately-labelled training examples, rapture makes use of both
symbolic and connectionist learning techniques for revising the rules,
in order that they correctly classify all of the training examples. a
modified version of backpropagation is used to adjust the certainty
factors of the rules, id3's information-gain heuristic is used to add
new rules, and the upstart algorithm is used to create new hidden
terms in the rule base. 

results on refining four real-world rule bases are presented that
demonstrate the effectiveness of this combined approach.  two of these
rule bases were designed to identify particular areas in strands of
dna, one is for identifying infectious diseases, and the fourth
attempts to diagnose soybean diseases.  the results of rapture are
compared with those of backpropagation, c4.5, kbann, and other
learning systems.  rapture generally produces sets of rules that are
more accurate that these other systems, often creating smaller sets of
rules and using less training time. 










 refinement of bayesian networks by combining connectionist and
symbolic techniques 
sowmya ramanchandran

ph.d. proposal, department of computer sciences, university of texas
at austin, 1995. 


bayesian networks provide a mathematically sound formalism for
representing and reasoning with uncertain knowledge and are as such
widely used. however, acquiring and capturing knowledge in this
framework is difficult. there is a growing interest in formulating
techniques for learning bayesian networks inductively. while the
problem of learning a bayesian network, given complete data, has been
explored in some depth, the problem of learning networks with
unobserved causes is still open. in this proposal, we view this
problem from the perspective of theory revision and present a novel
approach which adapts techniques developed for revising theories in
symbolic and connectionist representations.  thus, we assume that the
learner is given an initial approximate network (usually obtained from
a expert). our technique inductively revises the network to fit the
data better.  our proposed system has two components: one component
revises the parameters of a bayesian network of known structure, and
the other component revises the structure of the network. the
component for parameter revision maps the given bayesian network into
a multi-layer feedforward neural network, with the parameters mapped
to weights in the neural network, and uses standard backpropagation
techniques to learn the weights. the structure revision component uses
qualitative analysis to suggest revisions to the network when it fails
to predict the data accurately. the first component has been
implemented and we will present results from experiments on real world
classification problems which show our technique to be effective.  we
will also discuss our proposed structure revision algorithm, our plans
for experiments to evaluate the system, as well as some extensions to
the system.










revising bayesian network parameters using backpropagation
sowmya ramachandran and raymond j. mooney

proceedings of the international conference on neural
networks (icnn-96), special session on knowledge-based artificial
neural networks, washington dc, june 1996. 


the problem of learning bayesian networks with hidden variables is known to
be a hard problem. even the simpler task of learning just the conditional
probabilities on a bayesian network with hidden variables is hard. in this
paper, we present an approach that learns the conditional probabilities on
a bayesian network with hidden variables by transforming it into a
multi-layer feedforward neural network (ann). the conditional probabilities
are mapped onto weights in the ann, which are then learned using standard
backpropagation techniques. to avoid the problem of exponentially large
anns, we focus on bayesian networks with noisy-or and noisy-and
nodes. experiments on real world classification problems demonstrate the
effectiveness of our technique.










  comparing methods for refining certainty factor rule-bases   

j. jeffrey mahoney and raymond j. mooney  

 proceedings of the eleventh international workshop on machine
learning, pp. 173-180, rutgers, nj, july 1994. (ml-94) 


this paper compares two methods for refining uncertain knowledge bases using
propositional certainty-factor rules.  the first method, implemented in the
rapture system, employs neural-network training to refine the certainties
of existing rules but uses a symbolic technique to add new rules.  the second
method, based on the one used in the kbann system, initially adds a
complete set of potential new rules with very low certainty and allows
neural-network training to filter and adjust these rules.  experimental results
indicate that the former method results in significantly faster training and
produces much simpler refined rule bases with slightly greater accuracy.









  modifying network architectures for certainty-factor rule-base revision
   
j. jeffrey mahoney and raymond j. mooney  

 proceedings of the international symposium on integrating
knowledge and neural heuristics 1994, pp. 75-85, pensacola, fl,
may 1994. (isiknh-94) 

 
this paper describes rapture --- a system for revising
probabilistic rule bases that converts symbolic rules into a
connectionist network, which is then trained via connectionist
techniques.  it uses a modified version of backpropagation to refine
the certainty factors of the rule base, and uses id3's
information-gain heuristic (quinlan) to add new rules.  work is
currently under way for finding improved techniques for modifying
network architectures that include adding hidden units using the
upstart algorithm (frean).  a case is made via comparison with fully
connected connectionist techniques for keeping the rule base as close
to the original as possible, adding new input units only as needed.











  combining connectionist and symbolic learning to refine certainty-factor 
rule-bases   

j. jeffrey mahoney and raymond j. mooney  

 connection science, 5 (1993), pp. 339-364. (special issue on
architectures for integrating neural and symbolic processing) 


this paper describes rapture --- a system for revising probabilistic knowledge
bases that combines connectionist and symbolic learning methods. rapture uses
a modified version of backpropagation to refine the certainty factors of a
mycin-style rule base and it uses id3's information gain heuristic to add
new rules.  results on refining three actual expert knowledge bases demonstrate
that this combined approach generally performs better than previous methods.











  combining symbolic and neural learning to revise probabilistic theories   

j. jeffrey mahoney & raymond j. mooney  

 proceedings of the 1992 machine learning workshop on integrated
learning in real domains, aberdeen scotland, july 1992. 


this paper describes rapture --- a system for revising probabilistic
theories that combines symbolic and neural-network learning methods. 
rapture uses a modified version of backpropagation to refine the certainty
factors of a mycin-style rule-base and it uses id3's information gain heuristic
to add new rules.  results on two real-world domains demonstrate that this
combined approach performs as well or better than previous methods.







estlin@cs.utexas.edu