"drop-in" publishing with the world wide web




"drop-in" publishing with the world wide web

jim davis and carl lagoze
xerox inc. and cornell university


abstract


   the goal of drop-in publishing is to simplify digital publishing
   over the internet.  we would like digital publishing of
   non-commercial matter (e.g. technical reports, course notes,
   brochures) be as easy as sending email is now, but with the virtues
   of archival storage and easy searching that we associate with
   electronic libraries.  we propose a protocol, dienst, to allow
   communication between clients and document servers by encoding
   object-oriented messages within url's.  a preliminary version of
   this protocol now runs at eight sites, and we describe some of its
   features.  next we present tools for automating the maintenance of
   document collections.  finally, we discuss the problems we've
   had with the web as it stands, hoping to motivate changes that
   would improve performance of digital library systems such as ours.



a library with no limits...



"however one may sing the praises of those who by their virtue either
defend or increase the glory of their country, their actions only
affect worldly prosperity, and within narrow limits....[but] aldus is
building up a library which has no other limits than the world itself."



desiderius erasmus wrote these words in praise of his friend aldus, a
book publisher of the 16th century.  more than 400 years later,
digital publishing may finally enable us to fulfill this vision,
providing universal access to all the world's information.  what's in
the way?



the existing technologies (www, gopher, and even anonymous ftp) make
reproduction and transmission fairly fast and cheap, but do little or
nothing to help writers write or readers find or read documents.  in
our view, the problem is that they provide too little structure to the
document collection.  all of them present basically the same
abstraction, namely a hierarchy of files, but do nothing to help the
user locate a file within a hierarchy.  every site is different. some
group reports by year, others by project name; but even if every site
on the internet organized its hierarchy identically, it would not be
enough, because every site also has its own conventions for naming
files, indicating data formats, and making searchable indices.  a
writer who wishes to contribute has basically the same problem - it's
easy to copy a file into an anonymous ftp area, but hard to make sure
that it's indexed properly.  a considerate writer might want to
provide the same document in several formats, to increase the chances
of accessibility, but this is a nuisance.  we claim what's needed is a
new, higher level protocol that hides the underlying details, and
the underlying tools to simply library management.



this paper presents our first steps towards the universal
library.  we describe a protocol for universal access and the server
that implements it.  (for those familiar with our server - in this
paper we describe not the currently running protocol, but rather the
one we have submitted as an internet draft 
[dienstprot] , which corrects  a number of
design flaws in the working version.  we regret any confusion this
causes.)  we present a number of tools that integrate with our server
to make publishing a document on-line relatively easy.  we also
discuss the steps we took to bring a large, existing collection online
from paper.  finally, since our protocol is based on the world wide
web, we also describe some of the problems we've observed in using it,
in the hope that others at this conference will have solutions we can
adopt.



our focus on non-commercial publishing requires explanation.  we
realize that some content providers will not place their intellectual
property on the net until clear definitions of legal rights and
mechanisms for payment and protection are in place.  we have nothing
to contribute in these areas.  nevertheless there are a number of
providers, such as universities or corporate internal groups, for whom
these issues are less pressing, and we believe that we can thus make
some useful contribution without working on the additional issues
raised by economics.




dienst provides a uniform protocol for document access

dienst is a protocol for search, retrieval and display of documents.
dienst models the digital library as a flat set of documents, each of
which has a unique name, can be in many formats (e.g., tiff, gif,
postscript) and consists of a set of named parts.



dienst supports a message-passing interface to this document model.
messages may be addressed to every document server, to a particular
server, to one document, or to a particular part of a document.  a
message is encoded into the "path" portion of a url, and contains the
name of the message, the recipient, and the arguments, if any.  a
message may be sent to any convenient dienst server (the nearest, for
example), which will execute it locally if or forward it as
appropriate.  dienst appears to be a single virtual document
collection, and hides the details of the server distribution.  (note
that the actual implementation does not use an
object oriented language, we use message passing only as a convenient
conceptual model.)

 each document in dienst has a unique identifier which names each
document in a location-independent manner.  this identifier, called a
docid, serves exactly the same role as a urn, and when urns are
fully specified we will adopt them.  a docid has three components: a
naming convention, a publisher and a number.  to
ensure that each docid is unique, each component (or rather, the
institution that issues each component) guarantees that the next
component is unique - thus each naming convention controls a namespace
of publishers, and each publisher issues a set of numbers.

 for each publisher, there must be at least one server to handle
messages for the documents issued by that publisher.  in our view, the
minimum commitment a publisher must make to issue a document is to
store and deliver the document to the network.  when a dienst server
receives a message for a document it locates the closest server for
the document's publisher and forwards the message to it.


 dienst messages address four types of digital library services:
user interface services which present library information in a
format designed for human readability, repository services,
which store the document, and support retrieval of all or part,
index services, which provide search, and miscellaneous
services, which provide general information about a server.



of these four services, only the first is used directly by a human.
the others used by programs, in particular other dienst servers, but
also by other digital library or publishing systems.  for example, the
stanford information filtering tool ([sift])
obtains bibliographic records through the index interface, and we are
currently designing a gateway to the waters ([waters]) system.  we encourage other developers of
digital library systems to provide both user-interface and
application-interfaces to their systems.



all services except the last are optional at a given site.  this
allows maximal flexibility in the way that particular server
implementations interoperate.  for example, one server may exist
solely as a user interface gateway, providing transparent access for
users to a particular domain of indexes and repositories.  we see this
flexible interoperability as key to the development of a digital
library infrastructure where the "collection" will span multiple sites
and continents.

repository servers store documents in multiple formats

a key difference between dienst and other current digital library
systems is its ability to represent documents in multiple formats.
most current digital libraries present documents in exactly one form,
postscript.  although postscript is almost always available for newly
produced documents, there are problems with relying on it to the
exclusion of all other formats.  first, most older works are only
available in paper, making scanned page images the only practical
means of bringing the material online.  (we describe our experiences
in doing that below.)  second, looking forward we can expect to see
other document representations become popular.  (surely at a world
wide web conference we can claim that html will be used.)  a third
reason is that for some applications, other formats are just better.
for example, if one wishes to do full text indexing on a document
collection, the plain text is  more useful than the postscript
file, and if one wishes to display just a single page, a collection of
page images may be better than searching through postscript.
therefore, dienst's conceptual data model, allows each document to be
stored in one or more formats.



the dienst protocol includes a message that requests a document for a
list of formats in which it is available.  we specify formats with
mime ([mime]) content-types.  dienst does not
support the notion of explicit conversion between document formats (as
does system 33 [putz]).  a repository willing and
able to provide a document in a given format should simply list that
format, even if it is only obtained through a conversion service.




diversity is the rule on the internet, and each site supporting
dienst is likely to store their documents in a different way.  the
dienst protocol hides all detail of the underlying storage
organization -- this is in sharp contrast to ftp, gopher, and "bare"
http, where the underlying hierarchy is visible.  each dienst
repository includes a function which maps from a docid and format to
the actual storage pathname on that server.  this hides both details
of file system structure and file typing or naming conventions from
outside users.  thus one may request, say, the second page of the tiff
version of a document from a server without needing to know where
and how it is stored.


index servers support search

an index server accepts queries (in some query language) and searches
for document records that satisfy the query.  in our model, an index
server is totally distinct from a repository.  repository data is likely
to be huge, but index servers store only meta-data, which is quite
modest in size.  the choice of a query language is crucial to 
the power of an index server.  as we did not wish to make this choice,
the dienst protocol is designed with one initial query language,
and provision for extension to support others.  



every query language is based on an underlying model for the meta data
it queries.  the initial query language in dienst assumes a minimal
data model, where documents have an author, title, and abstract in
addition to the publisher and number.  a query may refer to any of
these fields; if it refers to more than one then the terms are
connected with an implicit "and".  thus one might query for all
documents published by author "wilson" at publisher "stanford".

 a search request returns a document of type
text/x-dienst-response, consisting of records containing
meta-information on all the matching documents.  this meta-information
follows the encoding proposed for uniform resource characteristics
(urc)  [urc] .  the urc draft proposes fields such
as title, author and content-type and url, all of which which are
obviously applicable; we have added a number of experimental attributes.

a prototype implementation runs at eight sites

an initial version of dienst and a prototype implementation were
developed as part of the computer science technical report (cstr)
project, an arpa-sponsored, cnri-directed effort to create an online
digital library of technical reports from the nation's top computer
science universities.
this version was installed at the five universities that form
the project
(cornell,
cmu, berkeley, mit, and stanford),
and shortly thereafter at princeton, dartmouth, and rochester.
here we describe a few of its features.
a full account may be found in  [dienst].



one uses dienst by connecting to any convenient dienst server (that
supports the user interface services) using a standard web client.
this server will display a form for searching the collection.  unless
the user restricts the search to a single publisher, all dienst
servers are searched in parallel.  each dienst server is made aware of
all other dienst servers by fetching a list of all servers from a
single, central meta-server.  thus when a new server comes online,
other servers become aware of it after only a short time.  the results
from a search are displayed as a list of the docid, author, title, and
date for each matching document, and include a url for each document.
selecting one displays the document in more detail, including a list
of the available formats (obtained as described above.)  the user can
retrieve the document in any of the formats.

 some repositories include page images as 4-bit 72 dot per inch gif
files.  when this is the case, the user interface service is able to
display the document page at a time, inline on the user's web client.
we found that such pages are readable on most monitors and saves
considerable network bandwidth compared to the 600 dpi tiff images.
in addition, some sites also store reduced size "thumbnail" page
images, which allow the user to quickly browse through a document and
then click to view a interesting page (say one that contains a
graphic) in full-page version.  although we do not have any formal
user studies, anecdotal evidence says that this is a very powerful
and helpful feature.  



the server also allows the user to download and/or print all or
selected pages of the document.  local users may print directly, while
remote users can download a postscript version of the document and
then print it manually.  since all documents are not available in
postscript, the server has the ability to translate from tiff images
to level 2 postscript on the fly.

maintaining the document collection

our goal is to simplify the process by which an author publishes
digital documents.  much of the work in this area is at the
document creation layer - that is, enhancements to html and/or html
editors.  our approach is to allow authors to use their traditional
text production system - latex, troff, word, etc - and then provide
tools by which they can submit the results of that text processing to
a digital library

 dienst simplifies digital library maintenance

digital library technology will only propagate beyond the
technologically savvy if such systems require minimal human
intervention, especially by trained experts.  two points are obvious.
first, authors are concerned primarily with writing documents and
getting them published.  submission to a digital library should
require little more skill than using a word processor.  second, many
of the organizations that wish to publish documents (e.g., government
agencies, academic departments, small companies) have little technical
expertise.  these organizations might tolerate the need for a
reasonable skill level to install a digital library system (we intend
to address the skill level required to install the digital library
system in future work).  however, they surely will not tolerate the
cost of a systems expert to maintain the library.



at cornell we have implemented a set of tools that mostly automate the
process of managing a digital library.  the tools are closely
integrated with the dienst digital library server.  they are similar
in spirit to those implemented for the wide area technical report
server ([waters]) system, known as techrep, but
whereas techrep is designed to maintain the centralized index and
unstructured ftp-based document repository that is characteristic of
waters, the tools described here are tailored for the distributed
indexes and structured repositories characteristic of dienst.



our design goal was to make the digital library maintainable by a
document librarian (dl) with relatively low-level computer training.
this dl serves four major roles - 1) as the general manager of the
collection; 2) as the reviewer of
document submissions, to protect against counterfeit document
submissions; 3) as the clearing house for copyright issues; and 4) as the archiver of document hardcopy.  this system has
recently been installed in the cornell computer science department and
is now the means for all technical report submissions.



authors add documents with an html form

the  submitter  prepares a document for submission by producing a
postscript representation.
rather
than a plethora of document formats from a variety of word processors,
we determined that postscript represents a 
 
lingua franca 
 
that
could be generated from virtually all word or text processing systems.
we recognize that there will be documents that can not be represented
in this fashion, but estimate that there number will be very few and
that techniques for managing them can be developed as the process
matures.  



the author submits a document by completing an html form that contains
text fields for bibliographic data about the document.  these fields are the
document title, author(s), pathname of the postscript file, abstract,
and submitter's e-mail address.  the submitter can quickly complete this form by "cutting and pasting" text from the document source.



the document librarian validates submissions to the library

the document librarian, in the role of gatekeeper of the system,
learns of each submission through an automatically generated e-mail
message. no document actually enters the database until the dl
manually checks the submission.  in addition, the dl acts as the legal
gateway, ensuring that the authors complete a copyright release form
that gives the department permission to make the document available
over the internet.  when manual checking and copyright clearing are
complete, the dl uses a simple command to assign a docid to the
document and signal that the document is ready entry into the
database.



the remainder of the process is fully automated.  software that is
integrated with the digital library server
generates the rfc-1357 bibliographic entry from the
submitter's entry, checks the validity of the postscript file, builds the
actual database entry, and generates the gif images for online viewing
and browsing of the document.



the image conversions in this process are done with the extended portable bitmap toolkit ([pbmplus]).  pbmplus
consists of a number of filters for conversion between a variety of
image formats 
(tiff's, gif's, x bitmaps, etc.) and a small set of portable formats
, and a set of tools to perform 
manipulations (rotations, color transformation, scaling) on the
portable format files.  pbmplus has the advantages of being free,
quite reliable, usable on a wide variety of graphical formats, and
quite powerful in its basic image manipulating capabilities.  

document librarian controls document withdrawal

a library system must be able to handle author requests for
document withdrawal.  the reason for withdrawal may be invalidation of
the published research or newly published results in another document.
for purposes of maintaining the integrity of collection, we have made the
document librarian the control point for this operation.
document withdrawal, via a simple command, replaces the bibliographic
file with an entry whose only attributes are the document number and a
"withdrawn" flag - all other bibliographic information is deleted.
this ensures that the docid is not reused for another document.
furthermore, the withdrawal moves the original bibliographic
file and associated  image and postscript
files  to a location that is not accessible to the document server.

hardcopy is sometimes required

while electronic document delivery is the 

raison d'etre

of our system, we recognize that publication quality hardcopy
is sometimes needed.  the document librarian must produce
paper copy for archival storage and for people who
do not have electronic access.

in our system, printing of tr's is done using a
package provided by cornell information technologies called ez-publish
 [ezpub] .  ez-publish allows users across campus on various
platforms to print to a central xerox docutech publishing system.
this is a publication quality printer that offers very high-speed and
resolution (135 pages/minute, 600 dpi) and document setup facilities
such as binding, different paper types, etc.  
with a command in the
dienst document management suite the dl can specify that multiple
copies of a tr be printed on the docutech.  the command does automatic
setup of the print job including formatting of a standard cornell
technical report cover page.




we have just begun to use this automated system in the computer science
department at cornell.  at a later time we will evaluate the
effectiveness of the system, with special attention payed to the
number of documents that require a special submission procedure (i.e.,
are not translatable to postscript).  obviously if the ratio of these
is high to the number submitted documents, we need to rethink the
design of the system.

 digitizing existing documents is a mostly manual task

we describe above a system for almost complete automation of the
document submission process.  at cornell, we faced the additional task
of converting an existing collection to digital form.  while some of
the tools described above were useful for this task, a large amount of
manual intervention was required.

the cornell computer science department has been publishing technical
reports since 1968.  as of september, 1994 the department had
published 1449 tr's, with an average length of thirty-six pages (a
total of over 52,000 pages).  the digital record for many of
these tr's is either non-existent, not easily available, or in a
format that is difficult to interpret with current hardware and
software (for example, a document formatted in an extinct copy of
wordstar that is only available on floppies for a long-gone cpm system).  



the common form that exists for all existing documents is hardcopy -
the department maintains archival copies of the entire tr corpus.  a
production scanning facility on campus allowed the department to
convert the entire corpus to high-quality 600dpi group 4-compressed
tiff images.  over a nine month period all hardcopy pages were scanned to
individual tiff files and downloaded via ftp to disk in the computer
science department.  each tiff file ranges in size from around one
kilobyte for a blank page to almost two megabytes for a page that
contains a high quality photographic image. the total collection of
pages images now occupies around 3.6 gigabytes.



it should be noted that scanning a collection, even as modest as the
cornell cs tr's,  is time consuming, labor intensive, and not without
problems.  even the most careful scanning technician occasionally
misses pages, skews pages, or misses part of a page due to a unnoticed
fold when the page is put on the scanner bed.  these problems are
difficult, if not impossible, to detect automatically.  in addition,
any problems that are detected are computationally intensive to
correct.  for example, a simple ninety-degree rotation of a 600 dpi
tiff image (due to incorrect scanning orientation) can take up to
thirty minutes on a reasonably equipped sparcstation 10.



an example illustrates the difficulty of correcting scanning
problems.  we discovered after all scanning was complete that many of
our older tr's were scanned from pages that were oriented in landscape
mode - two pages side-by-side.  the result was a tiff file containing
two page images, which made correct page mapping impossible in the
document server.  while it was easy to find files with this problem
(by reading the height and width from the tiff header with a
publically available tiff package 
[leffler]),
reasonably quick correction required handcrafting c-code to
split the files.  even with the handcrafted code, the location and
correction process took over a week of compute time on a powerful workstation.



in addition to manual scanning of documents, we also had to manually
enter the rfc-1357 bibliographic files.  while it would have been easy to write
translators between rfc-1357 and other common bibliographic formats
such as bibtex, refer, etc, a consistent electronic bibliographic
format was not available for all the tr's.

 the web is an imperfect document viewing technology

basing our system on the world wide has had both benefits and
shortcomings.  the obvious benefit is wide availability over
publically available browsers.  the shortcoming is that html, http,
and web browsers lack a number of features important for digital
document display and navigation.  in this section we enumerate these
features with the goal of inspiring discussion and enhancement of the
technologies by the web community.

facilities for display of compound documents

the web has insufficient mechanisms for displaying documents that
consist of multiple textual and non-textual parts.  in the electronic
mail world, this issue is addressed by mime (multipurpose internet
mail extensions) [mime].  although http uses mime typing to allow
browsers to map to the proper viewer for a document, documents are
allowed to have only a single mime type - multipart mime is not one of
them.  the only facility for multi-format documents is the ability to
embed images (either gif or x bitmap) in an html document.  yet there
are gross inefficiency problems with image embedding since the http
browser must initiate an http get message for each embedded image.
for a document with many embedded images, this can lead to
unacceptable document download times.  furthermore, there are other
types that one might like to embed in documents; for example, 
mpeg clips. 

ability to display in-line tiff images

among the many digital image formats (gif, jpeg, pbm, etc.), tiff is
the most flexible and extensible.  the tiff specification is
constantly evolving with the latest being revision 6.0, finalized in
1992  [tiff] .  the most significant evolution,
from the standpoint of reducing network bandwidth in image transfers,
is the growing number of compression schemes for tiff images.  the
ability to display in-line tiff images in html documents would take
full advantage of this rich de-facto image standard and permit
immediate display of images produced by scanners, fax machines, and
most paint and photo-retouching programs without computationally
expensive conversion to gif format.

arbitrary rectangle section from the client

viewing of document images on the web would be greatly enhanced if the
user of a client were able to select across an arbitrary rectangle in
the image and transmit the selected coordinates back to the server.
the server could then retransmit a "zoomed" image of the selected
image, if the higher resolution were available (which it often is in a
high resolution tiff image).  image zooming is an important feature
when the image being viewed is a document page that contains figures
or tables with small fonts.

client feedback on display capabilities

a main contribution of dienst is that it supports the notion of
multiple formats for the same document.  the user can select among the
available formats and use the view appropriate for that format.  we
would prefer to, at the server end, chose the "best" format to display
on the respective client.  this would be possible if the client http
request contained information on the display capabilities of the
client system, especially display depth and size.

authentication

the ability to restrict who is able to access a document is an
essential feature of a production digital library.  while our system
is intended for non-commercial publishing, limiting access is required
even in this domain; say, for example, documents that should only be
read by members of a campus community or employees of a corporation.
to do this, we require that server be able to guarantee the identity
of those making protocol requests.


summary

we have described a system, dienst, that simplifies document
publishing on the internet.  this system makes two important
contributions.



first, dienst provides a uniform protocol for search, retrieval, and
display of documents.  this protocol addresses a flexible document
model where each document has a unique name, can be in multiple
formats, and consists of a set of named parts.  these parts can be
physical, such as pages, or logical, such as chapters and tables.  in
addition, the protocol allows full interoperability between
distributed digital library servers.  the result is that the user sees
a single virtual document collection.



second, dienst provides a set of tools that permit easy management of
a digital library.  these tools automate document submission, permit a
document librarian to manage the collection, and facilitate the
production of archival hardcopy.



we plan over the next year to build on this technology in a number of
ways. installation of the digital library server is too difficult.  we
intend to implement tools that will "auto-configure" the server.
the search engine in the current implementation is primitive.
we intend to include more advanced search engines, for example
full-text search,  to make document
discovery in a collection more powerful and easier.  the current
strategy of conducting a parallel search over all servers does not
scale over a very large number of servers.  we intend to use
meta-information about individual document servers to improve the
search strategy. with this facility, one could, for example,  choose to search only those
libraries that have a high probability of containing computer science
documents.  we plan to examine and possibility incorporate current work on
copyright servers, so dienst might be used for commercial documents.
finally, we hope to use some of the current work in
location-independent identifiers to refine the method by which
documents on the net are addressed in dienst.

acknowledgements

this work was supported in part by the advanced research projects
agency under grant no. mda972-92-j-1029 with the corporation for
national research initiatives (cnri).  its content does not
necessarily reflect the position or the policy of the government or
cnri, and no official endorsement should be inferred.  this work was
done at the design research institute, a collaboration of xerox
corporation and cornell university, and at the computer science
department at cornell university.

references

[cohen]

danny cohen.  a format for e-mailing bibliographic records rfc-1357 

[dienst]
james r. davis, carl lagoze. 
a protocol and server for a distributed
digital technical report library.
cornell university computer science department technical report 94-1418,
june 1994.


[dienstprot]
james r. davis, carl lagoze. 
dienst, a protocol for a distributed digital document
library. internet draft.

[ezpub]
cornell information technologies. 
how to use ez-publish and the docutech printer at
cornell information technologies. november 24, 1993.

 [leffler]
sam leffler. 
public tiff package. 
available via ftp from 
sgi.com/graphics/tiff/v3.2beta.tar.z .

 [mime]
nathaniel s. borenstein, ned freed.
mime (multipurpose internet mail extensions). 
 rfc-1521 .

 [pbmplus]
jef poskanzer.
extended portable bitmap toolkit. 
available from many anonymous ftp sites including
ftp.ee.utah.edu.


[putz]
steve putz
design and implementation of the system 33 document service
xerox parc

p93-00112, 1993

[sift]
online service
at http://sift.stanford.edu.

[tiff]
aldus corporation.  tiff revision 6.0 specification.


 [urc] 
michael mealling.  encoding and use of uniform resource
characteristics. 

internet draft.


[waters]
kurt j. maly, edward a. fox, james c. french and alan l. selman. 
wide area technical report server.
published online in http://www.cs.odu.edu/waters/waters-paper.ps


biographies

jim davis works for xerox at the design research institute, a
non-proprietary consortium at cornell university which which seeks
ways to improve the engineering design process.  he received a phd in
1989 from mit's media technology laboratory.  his thesis, the back
seat driver was a computer program which provided spoken
driving instructions to the operator of a car in real-time.  prior to
that, he worked in research and development at a number of places
including atari's cambridge research laboratory.  at the dri he works
on developing electronic corporate memory.  his most recent project
is a system for 
shared group annotation using the world wide web.  he also plays
electric bass and is learning dutch.



carl lagoze works for the  computer science department  at
 cornell university  as a senior
software engineer in the  cstr project .
he received a master of software engineering from the wang institute
of graduate studies in 1987. after receiving his degree he worked in
both academia and the commercial world developing tools for the
generation of language-specific editors.  over the past two years he
has discovered the joys of digital libraries and the fascinating world
of information capture and access.  from the view of his non-technical
friends, he is doing "something on that information superhighway."
mr. lagoze is also the proud parent of the cutest baby ever and an
avid cyclist and canoeist.


contact author: davis@dri.cornell.edu 607-255-1134