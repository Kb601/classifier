peter j. downey's home page






  peter j. downey 


peter j. downey is associate professor of 
computer science
at the 
university of arizona.
he received a ph.d. from harvard university in 1974, and joined
the faculty at the pennsylvania state university.
in 1978 he moved to arizona.

downey's main research area is probabilistic analysis of
algorithms and systems, with particular emphasis on modeling
the performance of parallel
algorithms.
these areas are described in detail below.

during the 1996-97 academic year, downey is on sabbatical
leave, but can be reached at the addresses below.



department of computer science 
gould-simpson building, room 739
the university of arizona 
po box 210077
tucson  az 85721-0077
phone: (520) 621-2207 
fax: (520) 621-4246 
pete@cs.arizona.edu






naval observatory time is now:   
 






parallel program performance

the goal of this research project is
the development of
analytic methods to evaluate the performance of parallel asynchronous
algorithms and systems.
in order to understand the origins and effects of overheads in parallel
computation, such as synchronization delays,
methods focus on probabilistic models of workloads and
task processing times.
rather than developing exact but unrealistic models with only narrow
applicability, emphasis is on the development of bounds and approximations
that are robust and valid for extremely general workload and task
distributions (distribution-free methods).
for large workloads and massive numbers of parallel processors, laws of
large number apply and allow run-time and speed-up to be approximated or
bounded in terms of a few underlying parameters of the workload.

analysis of overheads

analysis of parallel
algorithms is distinctly different from that of
sequential algorithms because of the presence of overheads,
following from the need to coordinate parallel activity.  overheads
are of two fundamentally different kinds.
explicit
overheads
result from the need to add additional code
to a parallel algorithm to handle coordination matters such
as forks and message sends.
implicit
overheads
arise from delays spent waiting on synchronization
events, such as joins,
to occur.
implicit and explicit overheads
increase with increasing degree of parallelism;
expected implicit overheads
increase with the mean, variance and skewness of the underlying task time
distributions.
much of this research aims
to quantify the effects
of such overheads,
and to understand how they depend on
various parameters of the workload distribution.
work on this subject includes

bounds and approximations for overheads in the time to join
parallel forks ,
 orsa journal on computing 7, 2(spring 1995), 125-139.

foundations: extreme order statistics

let x1,  ... , xn
be random
variables, representing execution times of tasks.
two random variables derived from the task times
play a critical
role in determining the performance of
parallel activity:


the extreme
mn = max ( x1,  ...  , xn)
and

the sum
sn = x1 + ... + xn.

work aimed at
understanding the
behavior of both
sn
and
mn for large
n,
even if
the
task times are dependent, forms a fundamental part
of this research upon which all analysis is based.
this foundational
theory must meet a very practical methodological constraint:
to be able to bound or approximate
the behavior of
extrema and sums
without detailed information about the task workload distributions.
work in this area includes
 the ratio of the extreme to the sum
in a random sequence with applications
,
with p.e. wright,
university of arizona technical report tr 94-18 (1994).

scheduling and sequencing

parallel performance issues occur at the operating systems level
in the design of multiprocessor schedulers.
the design and analysis of simple
scheduling policies having guaranteed
performance bounds is an important part of this research.
new applications of stochastic ordering relations to prove
optimality of static scheduling policies and
to develop bounds for dynamic policies are being
extended as part of this work.
new work in this area includes

scheduling independent tasks
to minimize the makespan on identical machines ,
with e.g. coffman, jr. and j. bruno,
probability in the engineering and informational
sciences 9, 3(fall 1995), 447-456.




[ top of page
|
department home page ]

http://www.cs.arizona.edu/people/pete/
last updated july 11, 1996