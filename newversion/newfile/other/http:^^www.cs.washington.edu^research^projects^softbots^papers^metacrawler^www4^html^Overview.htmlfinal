multi-service search and comparison using the 
        metacrawler


multi-service search and comparison using the 
        metacrawler
erik selberg
oren etzioni


abstract:
standard web search services, though useful, are far from ideal.
there are over a dozen different search services currently in
existence, each with a unique interface and a database covering a
different portion of the web. as a result, users are forced to
repeatedly try and retry their queries across different services.
furthermore, the services return many responses that are irrelevant,
outdated, or unavailable, forcing the user to manually sift through the
responses searching for useful information.

this paper presents the
metacrawler,
a fielded web service that 
represents the next level up in the information ``food chain.''  
the metacrawler provides a single, central interface for web document
searching.  
upon receiving a query, the metacrawler posts the query to
multiple search services in parallel, collates the returned references,
and loads those references to verify their existence and to ensure that
they contain relevant information. 
the metacrawler is sufficiently lightweight to reside on a
user's machine, which facilitates customization, privacy,
sophisticated filtering of references, and more.

the metacrawler also serves as a tool for comparison of diverse search
services. using the metacrawler's data, we present a ``consumer
reports'' evaluation of six web search services:
galaxy[5], 
infoseek[1], lycos[15], open
text[20], webcrawler[22], and
yahoo[9]. in addition, we also report on the most
commonly submitted queries to the metacrawler.


keywords:
metacrawler, www, world wide web, search,
  multi-service, multi-threaded, parallel, comparison


 introduction

web search services such as lycos and webcrawler have proven both
useful and popular.  as the web grows, the
number and variety of search services is increasing as well.  examples
include: the yahoo ``net directory''; the harvest home page search
service[7]; the query by image content 
service[12]; the virtual tourist[24], a directory
organized by geographic regions; and more.  since 
each service provides an incomplete snapshot of the web, users are
forced to try and retry their queries across different indices until
they find appropriate responses.  the process of querying
multiple services is quite tedious.  each service has its own
idiosyncratic interface which the user is forced to learn.
further, the services return many
responses that are irrelevant, outdated, or unavailable, forcing the
user to manually sift through the responses searching for useful
information.

this paper presents the metacrawler, a search service that attempts to
address the problems outlined above.  the premises
underlying the metacrawler are the following:
 no single search service is sufficient.  table
  2 shows that no single service is 
  able to return more then 45% of the references followed by users.

 many references returned by services are irrelevant and can be
  removed if the user is better able to express the query. table
  3 shows that up to 75% of the
  references returned can be removed if the user supplies a more
  expressive query.

 low-quality references can be detected and removed fairly
  quickly. table 4 shows that an average of about 100 
  references can be verified in well under 2.5 minutes, while
  simple collation and ranking takes under 30 seconds.

 these features will be used by the web's population. the
  metacrawler is receiving over 7000 queries per week, and that number is
  growing, as shown in figure 1.

 the metacrawler log
  facilitates an objective evaluation and comparison of the underlying
  search services. tables 5-8 detail
  trade-offs between the services. for example, lycos returns over 5%
  more followed references than any other service, yet webcrawler is
  the fastest, taking an average of 9.64 seconds to return answers to
  queries.



the metacrawler logs also reveal that people search for a
wide variety of information, from ``a. h. robins'' to ``zyx
music.'' while the most common queries are related to sex and
pornography, these only
account for under 4% of the total queries submitted to the
metacrawler as shown in table 1. nearly
half of all queries submitted are unique.

the remainder of this paper is organized as follows: the design and
implementation of the metacrawler are described in
section 2.  experiments to validate the above premises
are described in section 3.  we discuss related
work in section 4, and our ideas for future work and
potential impact appear in section 5. we conclude with
section 6.




 the metacrawler

the
metacrawler
is a free search service used for locating information
available on the world wide web. the metacrawler has an interface
similar to webcrawler and open text in that it
allows users to enter a search string, or  query, and returns a
page with click-able references or  hits to pages available on
the web. however, the internal architecture of the metacrawler is
radically different from the other search services.

standard web searching consists of three activities:
  indexing the web for new and updated pages, a process
  that demands substantial cpu and network resources.

  storage of the web pages retrieved into an index, which
  typically requires a large amount of disk space.

  retrieval of pages matching user queries. for most
  services, this  amounts to returning a ranked list of page
  references from the stored index.


standard search services create and store an index of
the web as well as retrieve information from that index. unlike these
services, the metacrawler is a  meta-service which uses no
internal database of its own; 
instead, it relies on other external search services to provide the
information necessary to fulfill user queries. the insight
here is that by separating the retrieval of pages from indexing and
storing them, a lightweight application such as the metacrawler can
access multiple databases and thus provide a larger number of
potentially higher quality references than any search service tied to
a single database.

another advantage of the metacrawler is that it does not
depend upon the implementation or existence of any one search 
service.  some indexing 
mechanism is necessary for the web. typically, this
is done using automated robots or spiders, which may not necessarily
be the best choice[13]. however, the underlying
architecture of the search services used by the metacrawler is
unimportant. as long as there is no central complete search service
and several partial search services exist, the
metacrawler can provide the benefit of accessing them simultaneously
and collating the results.

the metacrawler prototype has been publicly accessible since july 7, 1995.
it has been steadily growing in popularity, logging upwards of 7000
queries per week and increasing. the
metacrawler currently accesses six services: galaxy,
infoseek, lycos, open text, webcrawler, and yahoo. 
it works as follows: given a query, the metacrawler will submit that
query to every search service it knows in parallel. these services
then return a list of references to www pages, or hits. upon
receiving the hits from every service, the metacrawler  collates
the results by merging all hits returned.  duplicate hits are listed
only once, but each service that returned a hit is acknowledged.  expert
user-supplied sorting options are applied at this time.  optionally,
the metacrawler will  verify the information's existence by
loading the reference. when the metacrawler has loaded a reference, it
is then able to re-score the page using supplementary query syntax
supplied by the user.

when the metacrawler has finished processing all of the hits, the user
is presented with a page consisting of a sorted list of
references. each reference contains a click-able hypertext link to the
reference, followed by local page context (if available), a confidence 
score, verified keywords, and the actual url of the reference. each
word in the search query is automatically boldfaced. so that we may
determine which references are followed, each click-able link returned
to the user points not to the reference, but to a script which logs
that the reference was followed and then refers the user's browser to
the correct url.

querying many services and simply collating results will return more
results than any one service, but at the cost of presenting the user
with more irrelevant references. the metacrawler is designed to
increase  both the number of hits and relevance of hits
returned. the metacrawler yields a higher
proportion of  relevant hits by using both a powerful query
syntax as well as expert options so that users can more easily instruct
the metacrawler how to determine the quality of the returned
references. the query syntax used specifies required and non-desired
words, as well as 
words that should appear as a phrase. the expert options allow users
to rank hits by physical location, such as the user's country, as well
as logical locality, such as their internet domain. 



 user interface

while giving the user a web form with added
expressive power was easy, presenting the user with a form that
would facilitate  actually using the novel features of the
metacrawler proved to be a challenge. we strove for a balance between
a simple search form and an 
expressive one, keeping in mind interface issues mentioned by service
providers[23].

in our early designs, we focused
on syntax for queries with several additional options for improving
the result. this syntax was similar to infoseek's query syntax:
parentheses were used to define phrases, a plus sign designated a
required word, and a minus designated a non-desired
word. for example, to search for 
``john cleese,'' naturally requiring that both ``john'' and ``cleese''
appear together, the syntax required was the unwieldy  (+john
+cleese). not surprisingly, we discovered that while most users
attempted to use the  
syntax, they often introduced subtle syntactical errors
causing the resulting search to produce an entirely irrelevant set of
hits.

in our current design, we have reduced the need for extra syntax, and
instead ask the user to select the type of search. the three
options are:
  search for words as a phrase: 
  treat the query text as a single phrase, and attempt to match the
  phrase in pages retrieved, e.g. ``four score and seven years ago.''
  search for all words:  
  attempt to find each word
  of the query text somewhere in the retrieved pages. this is
  the equivalent of logical ``and.''
  search for any words:  
  attempt to find any word
  of the query text in the retrieved pages. this is the equivalent of
  logical ``or.''

the older syntax is still supported, although it is not advertised
prominently on the main search page, save for the minus sign, which
was the most widely used element of the query syntax. since we
changed the search page to this new design, the number of 
malformed requests has dropped significantly.

in addition to the query entry box, we maintain various expert options
which can be activated via menus. the metacrawler currently uses two
menus to provide extra expressiveness. the first describes a coarse
grain locality, with 
options for the user's continent, country, and internet domain, as
well as options to select a specific continent. the second menu describes
the sundry internet domain types, e.g.  .edu,  .com,
etc. these options allow users to better describe what they are
looking for in terms of where they believe the relevant information
will be.


 client-server design

current search services amortize the cost of indexing and storing
pages over hundreds of 
thousands of retrievals per day. in order to field the maximal number of
retrievals, services devote minimal effort to responding to each
individual query. increases in server capacity are quickly gobbled
up by increases in pages indexed and queries per day. as a result,
there is little time for more sophisticated analysis, filtering, and
post-processing of responses to queries.

by decoupling the retrieval of pages from indexing and storage, the
metacrawler is able to spend time performing sophisticated analysis
on pages returned. the metacrawler just retrieves data,
spending no time indexing or storing it. thus, the metacrawler is
relatively lightweight. the prototype, 
written in c++, comes to only 3985 lines of code including comments.
it does not need the massive disk storage to maintain an index nor
does it need the massive cpu and network resources that other services
require. consequently, a metacrawler client could reside comfortably
on an individual user's machine.

an individualized metacrawler  client that accesses multiple web
search services has a number of advantages.  first, the user's machine
bears the load of the post-processing and analysis of the returned
references.  given extra time, post-processing can be quite
sophisticated.  for example, the metacrawler could divide references
into clusters based on their similarity to each other, or it could
engage in  secondary search by following references to related pages
to determine potential interest.  second, the processing can be
customized to the user's taste and needs.  for example, the user may
choose to 
filter advertisements or parents may try to block x-rated pages.
third, the metacrawler could support scheduled queries (e.g., what's
new today about the seattle mariners?).  by 
storing the results of previous queries on the user's machine, the
metacrawler can focus its output on new or updated pages.  finally,
for pay-per-query services, the metacrawler can be programmed with
selective query policies (e.g., ``go to the cheapest service first''
or even ``compute the optimal service querying sequence'').

organizations may choose to have an institutional metacrawler with
enhanced caching capabilities, on the presumption that people within
an organization will want to examine many of the same pages.  the
cache could also facilitate local annotations, creating a
collaborative filtering and information exchange environment of the
sort described elsewhere [17].

finally, while our prototype metacrawler depends on the good will of
the underlying services, a metacrawler client would not.  in the
future, an underlying service may choose to block metacrawler
requests, which are easily identified.  however, it would be
nearly impossible to distinguish queries issued by a metacrawler
client versus queries made directly by a person.

 common usage

one of the frequently asked questions regarding search on the
internet is ``what are people searching for?'' table
1 summarizes the top ten repeated queries out of a
total of 50,878 queries made from july 7 through september 30. each
query in the top ten is related to sex. however, the combined top ten
queries amount only to 1716 queries out of 50,878 total queries, or
3.37%. further, 24,253 queries, or 46.67%, were not repeated.


table 1: top ten queries issued to the metacrawler



no.
query
times issued


1
sex
533


2
erotica
219


3
nude
217


4
porn
158


5
penthouse
127


6
pornography
112


7
erotic
105


8
porno
89


9
adult
89


10
playboy
67




``times issued'' lists the number of times the corresponding query was
issued from july 7 through sept. 30, 1995. note that while each query
is sexually related, the combined total amounts to less than 4\% of
the total queries processed by the metacrawler. also, 46\% of the
queries issued were unique.





 evaluation

the metacrawler was released publicly on july 7, 1995. averages and percentages
presented in this paper are based on the 50,878 completed queries,
starting on july 7 and ending september 30, except those in reference to
open text, which are based on 19,951 completed queries starting
september 8, when open text was added to the metacrawler's
repertoire. the log results from seven days were 
omitted due to a service changing its output format, causing that
service to return no references to the metacrawler even though the
service was available. the metacrawler is currently running on a dec
alpha 3000/400 under osf 3.2.

the first hypothesis we confirmed after we deployed the metacrawler
was that sending queries in parallel and collating the results was
useful. to confirm this, we used the metric that references followed from
the page of hits returned by the metacrawler contained 
relevant information. we calculated the  percentage
of references followed by users for each of the search services.
table 2 demonstrates the need for using multiple services;
while lycos did return the plurality of the hits that were followed, with
a 35.43% share (42.17% in the last month recorded), slightly under
65% of the followed references came from the other five
services. skeptical readers may argue that service providers
could invest in more resources and provide more comprehensive 
indices to the web. however, recent studies indicate the rate of web
expansion and change makes a complete index virtually
impossible[16].


table 2: market shares of followed references




% followed jul. 7 - sept. 30% followed sept. 8 - 30


lycos             35.43  42.17    


webcrawler        30.76  25.74    


infoseek          18.55  15.70    


galaxy            17.10  15.60    


open text           n/a  14.70    


yahoo             10.67   6.59    



this table shows the percentage each service has of the total followed
references. references returned by two or more services are included
under each service, which is why the columns sum to over 100\%.  the 
table demonstrates that a user who restricts his or her queries to a
single service will miss most of the relevant references.



we then analyzed the data to determine which, if any, of the added
features of the metacrawler were helping users. the metric we used was
the number of references pruned. table 3
shows the average number of references removed for each advanced
option.


table 3: effect of features in removing
irrelevant hits




feature   % of hits removed


syntax   39.79         


dead     14.88         


expert   21.49         




this table shows the percentage of hits removed when a particular
feature was used. ``syntax'' refers to queries that were removed
due to sophisticated query syntax (e.g., minus for undesired words);
``dead'' refers to unavailable or inaccessible pages, and ``expert''
refers to hits removed due to restriction on the references' origins.



using syntax for required or non-desired
words typically reduces the number of returned results by 40%. detecting dead
pages allowed the removal of another 15%. finally, the 
expert options were very successful in removing unwanted
references. when all of these features are used in conjunction,
up to 75% of the returned references can be removed.

 metacrawler benchmarks

we have shown that the metacrawler improves the quality of results
returned to the user. but what is the performance cost? table
4 shows the average times per query, differentiating
between having the metacrawler simply collate the results or verify them as
well.


table 4: average time for metacrawler return
  results




      wall clock time  user time  system time  lag time 


collated          25.70         0.32       1.87          23.51   


verified         139.30         22.72      4.50          112.08  




all times are measured in seconds. 
``wall clock time'' is the total time taken for an average
query, and is broken down into user, system, and lag time. ``user
time'' is the time taken by the metacrawler program, 
``system time'' the time taken by the operating system, and ``lag
time'' the time taken for pages to be downloaded off the network.



table 4 shows that the metacrawler finished relatively
quickly. the 
average time to return collated results is a little over 5 seconds
longer than the slowest service as shown by table
8. this is to be expected given the percentage of
the time a service times out, which causes the metacrawler to wait for
a full minute before returning all the results.
we are pleased with the times reported for verification. our
initial prototype typically took five minutes to perform
verification. we recently began caching retrieved pages for three
hours, and have found that caching reduces the average
verification time by nearly one-half. we are confident that this time can
be further reduced by more aggressive caching as well as 
improvements in the thread management used by the metacrawler.

since the metacrawler was publicly announced, the daily access count
has been growing at a linear rate. we are also pleased with an
increased use of the 
user options. figure 1 plots
the data points for the weeks beginning july 7 until september
30. ``feature use by week'' shows the number of queries where any of
the metacrawler's advanced features, such as verification, were used.

figure 1: queries per week from july 7 - sept. 30


 search service comparison

in addition to validating our claims, the metacrawler's logs also
allow us to present a ``consumer reports'' style comparison of the search
services. we evaluate each service using three metrics:
  coverage: how many hits will be returned on average?
  relevance: are hits returned actually followed by users?
  performance: how long does each service take, and how often does
  it time out?

 coverage

given a pre-set maximum on the number of hits returned by each
service, we measured both the percentage of references returned as
well as references unique to the service that returned them. thus,
75% returned with 70% unique shows that on average a service
returns 75% of its maximum allowed, with 70% of those hits being
unique.

tables 5 and 6 details our findings
in terms of raw 
content. it shows that with default parameters, open text returns
80% of the maximum hits allowed, with nearly 89% of those hits being
unique. lycos and webcrawler follow, also returning over 70%,
with slightly over 90% of those hits being unique. yahoo 
has particularly poor performance on the total hits metric, but this was not
surprising to us. we included yahoo on the hypothesis that people search
for subjects, such as ``mariners baseball,'' which yahoo
excels at. however, it turned out this hypothesis was incorrect, as
people tended to use the metacrawler to search for nuggets of information, such as ``ken
griffey hand injury.'' yahoo does not index this type information, and thus
shows poor content. presumably, most topic searches go to yahoo directly.

although each service returns mostly unique references, it is not
clear whether those references are useful. further, unique references are not necessarily unique to a
service's database, as another service could return that reference
given a different query string. 


table 5: returned references by service




            % of max hits returned  ave. hits returned / maximum allowed 


open text   80.0            8.0 / 10          


lycos       76.3           19.1 / 25          


webcrawler  70.2           17.6 / 25          


galaxy      56.9           28.4 / 50          


infoseek    43.5            4.4 / 10          


yahoo       11.1           11.1 / 100         




the first column shows the percentage of the maximum hits allowed that
each service returned. each percentage was calculated by dividing the
average hits 
returned by the maximum allowed for that service, as shown in the
second column. this percentage is a measure of how many hits a
service will provide given a pre-set maximum. the metacrawler used
different maximum values for services, as some had internal maximum
values, and others would either accept only certain maximums or none
at all. 



table 6: unique references by service




% of unique hits returned          


galaxy       99.6            


yahoo        92.8            


lycos        90.6            


webcrawler   90.3            


open text    88.8            


infoseek     79.5            




this table shows the percentage of references each service
returned that were unique to that service. 


 relevance
to measure relevance, two metrics are used. the first is which
service is returning the most references that people follow. this is
shown by table 2. the second metric is what percent
of references returned by each service are people following. 
table 7 summarizes these
calculations. it shows that nearly 6% of all references returned by
infoseek were followed. lycos, open text, and
webcrawler follow, each having about 2.5% of their hits
followed.

this data has two caveats. the first is that the relevant
information for people may be the list of references itself. for example,
people who wish to see how many links there are to their home page may
search on their own name just to calculate this number. the second
caveat is that these numbers may be skewed by the number of hits
returned by each service. thus, while infoseek has nearly 6% of its
results followed, only a total of 13,045 references returned by
infoseek were followed, compared with the 24913 references followed
that were contributed by lycos, which on average had only 2.5% of its
19 hits examined.


table 7: relevance of returned hits by service





 % of hits returned that are followed  total hits followed 


infoseek     5.89                 13,045      


lycos        2.56                 24,913      


open text     2.51                  4,025      


webcrawler   2.42                 21,631      


yahoo        1.33                  7,503      


galaxy       0.83                 12,022      




this table shows the percentage of followed hits for each
service. references returned by multiple services
are counted multiple times. column 2 shows the actual number of
references followed for each service. these numbers are out of
50,878 queries, except open text which is out of 19,951 queries. 


 performance
finally, we measure each service's response time. table
8 summarizes our findings. it is not surprising,
although disappointing, to find that average times vary from just
under 10 seconds to just under 20. the percent of time the services timed out
is under 5% for all services except open text, which is the newest
and presumably still going through some growing pains. one explanation
for the length of times taken by these services is that the majority
of requests are during peak hours. thus, results are naturally
skewed towards the times when the services are most loaded. times
during non-peak hours are much lower.


table 8: performance of services




              ave. time (sec)   % timed out 


webcrawler     9.64               2.30       


infoseek      12.30               3.01       


open text      16.26              14.13       


yahoo         18.32               2.28       


lycos         18.99               4.87       


galaxy        19.52               3.10       




this table shows the average time in seconds taken by each service to fulfill a
query. the second column gives the percent of time that the service
would time out, or fail to return any hits under one minute. 




 related work

unifying several databases under one interface is far from novel. many
companies, such as pls[21],
lexis-nexis[14], and verity[30]
have invested several 
years and substantial capital creating systems that can handle and
integrate heterogeneous databases. likewise, with the emergence of
many internet search services, there have been many different efforts
to create single interfaces to the sundry databases. perhaps the most
widely distributed is cusi[18], the configurable unified
search index, which is a large form which allows users to select one
service at a time and query that service. there are also several other
services much like cusi, such as the all-in-one search
page[2], or 
w3 search engine list[19]. unfortunately, while the
user has many services on these lists to choose from, there is no
parallelism or 
collation. the user must submit queries to each service individually,
although this task is made easier by having form interfaces to the
various services on one page.

the harvest system[6] has many similarities to the
metacrawler; however, rather than using existing databases as they are
and post-processing the information returned, harvest uses ``gatherers''
to index information and ``brokers'' to provide different interfaces
to extract this information. however, while harvest may have many
different interfaces to many different internal services, it is still a
search service like lycos and webcrawler, instead of a meta-service like
metacrawler.

there are also other parallel web search services. sun microsystems
supports a very primitive service[27], and ibm
has recently announced infomarket[11] which, rather
than integrating similar services with different coverage,
integrates quite different services, such as
dejanews[26], a usenet news search service, 
mckinley[29], a clone of yahoo with some editorial
ratings on various pages, in addition to open text and yahoo.

the closest work to the metacrawler is savvysearch[3], an
independently created multi-threaded search service released in may
1995. savvysearch has a larger repertoire of search services, although
some are not www resource services, such as roget's
thesaurus. savvysearch's main focus is categorizing users' queries,
and sending them to the most appropriate subset of its known
services[4].  


like the metacrawler, the internet softbot[8] is a
meta-service that leverages existing services and collates their results.  
the softbot enables a human user to state
what he or she wants accomplished.  the softbot attempts to
disambiguate the request and to dynamically determine how and where
to satisfy it, utilizing a wide range of internet services.  unlike
the metacrawler, which 
focuses on indices and keyword queries, the softbot accesses
structured services such as netfind and databases such as inspec.  the
softbot explicitly represents the semantics of the 
services, enabling it to chain together multiple services in response
to a user request.  the softbot utilizes automatic planning technology
to dynamically generate the appropriate sequence of service accesses.
while the metacrawler and the softbot rely on radically different
technologies, the vision driving both systems is the same.  both
seek to provide an expressive and integrated interface to internet
services.


 future work

we are investigating how the metacrawler will scale to use new
services. of particular importance is how to collate results returned
from different types of internet indices, such as usenet news and web
pages. also important is determining useful methods for
interacting with specialized databases, such as the internet movie
database[28]. if the information requested is
obviously located on 
some special purpose databases, than it does not make sense to query
each and every service. we are
investigating methods that will enable the metacrawler to determine
which services will return relevant data based solely on the query text
and other data provided by the user.


 future design

the existing metacrawler prototype can cause a substantial network load when
it attempts to verify a large number of pages. while one query
by itself is no problem, multiple queries occurring at the same time
can cause the system and network to bog down. however, with the
emergence of universally portable internet-friendly languages, 
such as java[10] or magic
cap[25], load problems can be lessened
by having users' machines take on the workload needed to perform their
individual query, as discussed in section
2.2. the javacrawler, a prototype next
generation 
metacrawler written in java, supports most of the  
features already present in the metacrawler. however, instead of users
running queries on one central service, each user has a local copy of
the javacrawler and uses that copy to directly send queries to
services. the load caused by verification will be taken by the user's
machine, rather than the central server. this has
the added benefit of inserting downloaded pages into the local cache, making
retrieval of those pages nearly instantaneous. the javacrawler is loaded
automatically from the metacrawler home page when visited with a
java-compatible browser.

 impact on search service providers

we anticipate that a wide range of
meta-services like the metacrawler will emerge over the next few
years. however, it is 
uncertain what the relationship between these meta-services and search
service providers will be. we envision that this relationship will hinge on
what form the ``information economy'' used by service providers takes.
we discuss two different models.

 charge-per-access

in the charge-per-access model, service providers benefit from
any access to their database. 
infoseek has already taken this model with their commercial
service. infoseek is financially rewarded regardless of who or what
sends a query to their commercial database. many other databases, both
on and off the web, also use this model.

the metacrawler fits in well with this model.
since service providers benefit from any access, the added
exposure generated by the metacrawler is to their advantage. further,
this model creates an implicit sanity check on the claims this paper
makes on the use of its features. in order for the metacrawler, or any
meta-service, to survive in such an economy, it must charge more per
transaction than the underlying services, as the metacrawler will in
turn have to pay each service for its information. thus, users must
be willing to pay the premium for the service. by voting with their
pocketbook, they can determine if those features are truly desirable.

 advertising

in the advertising model, service providers benefit from sponsors who
in turn gain benefit from exposure provided by the service.
nearly all major search services that do not charge users directly have
adopted this model, as have many other unrelated services which are
heavily accessed.

under this model, the providers' relationship with the
metacrawler can become problematic as the metacrawler filters away
superfluous information such as advertisements. one promising method
to ensure profitable co-existence
is to use provider-created interfaces. providers could create
an interface for the metacrawler to access their service which, in
addition to returning relevant hits, also 
returns the appropriate advertisement. another solution involves the
metacrawler accepting advertisements, and forming a
profit-sharing relationship with the service providers. we are
currently investigating these and other methods of mutually beneficial
co-existence with service providers.


 conclusions

in this paper we have presented the metacrawler, a meta-service for web
searching with additional features designed to return more references
of higher quality than standard search services. we demonstrated that
users follow references reported by a variety of different search
services, confirming that a single service is not sufficient (table
2).  further, due to the expressive power of the
metacrawler's interface, the metacrawler was able to automatically
determine that up to 75% of the hits returned can be discarded. finally, the
performance benchmarks and usage logs also show that the features
provided by the metacrawler are both reasonably fast and actually used
in practice.

the metacrawler provides a ``consumer reports'' of sorts for web searchers.
the individual service data extracted from the metacrawler's
logs is compelling evidence concerning the quality of each service. by
comparing services using the same query text and recording what
links users follow, we are able
to evaluate the services from a user's point of view. as far as we
know, we are the first to quantitatively compare the
search services used by metacrawler on a large sample of authentic user
queries.

while it is possible that some metacrawler features could be
integrated into the search services, others are intrinsic to
meta-services.  by definition, only a meta-service can provide the
coverage gained by using multiple services. also, as argued earlier,
client-side meta-services can offer user and site customizations, and
absorb the load caused by post-processing of search results. finally,
there are some features that do not belong under control of search
services for purely pragmatic reasons. for example, as more commercial
search services become available, tools will emerge that select which
services to use on the basis of cost.  an impartial meta-service such
as the metacrawler avoids the conflict of interest that would arise if
such a tool were offered by one of the commercial services.

new web services are constantly being created. as the number and
variety of services grows, it is natural to group existing services
under one umbrella.  the metacrawler goes further than merely
organizing services by creating an integrated  meta-service that
moves the interface (and the associated computational load) closer to
the user.  we believe that this trend of moving up the information
``food chain'' will continue.  the metacrawler is one of the first
popular meta-services, but many more will follow.

 acknowledgments

the research presented in this paper could not have been accomplished
without the help of many individuals.  we would like to thank mary
kaye rodgers, for editing assistance and for putting up with late
nights. ruth etzioni and ellen spertus provided comments on an earlier
draft.  dan weld, rich segal, keith golden, george forman, and donald
chinn were very vocal and active in testing the early prototypes of
the metacrawler, and craig horman and nancy johnson burr were
extremely helpful and patient in dealing with it when it ran
amok. lara lewis was very helpful in finding references upon
demand. the internet softbot group provided early insight into
desirable features of the metacrawler, and brian bershad and hank levy
contributed ideas relating to the impact the metacrawler could have on
the web. ken waln aided in early development for his form patches to
the www c library, and lou montulli helped in later development by
unlocking the secrets of  nph-scripts and netscape caching.
metacrawler development was supported by gifts from us west and
rockwell international palo alto research. etzioni's softbot research
is supported by office of naval research grant 92-j-1946 and by
national science foundation grant iri-9357772.


references

1
infoseek corporation.
 infoseek home page.
  url: 
  http://www.infoseek.com.

2
william cross.
 all-in-one internet search page.
  url:  http://www.albany.net/&#126;wcross/all1srch.html.

3
daniel dreilinger.
 savvy search home page.
  url:  http://www.cs.colostate.edu/&#126;dreiling/smartform.html.

4
daniel dreilinger.
 integrating heterogeneous www search engines.
  url: 
  ftp://132.239.54.5/savvy/report.ps.gz, may 1995.

5
einet.
 galaxy home page.
  url: 
  http://galaxy.einet.net/galaxy.html.

6
c. mic bowman et al.
 harvest: a scalable, customizable discovery and access system.
 technical report cu-cs-732-94, department of computer science,
  university of colorado, boulder, colorado, march 1995.
  url: 
  http://harvest.cs.colorado.edu/harvest/papers.html.

7
michael schwartz et al.
 www home pages harvest broker.
  url: 
  http://town.hall.org/harvest/brokers/www-home-pages/.

8
o. etzioni and d. weld.
 a softbot-based interface to the internet.
  cacm, 37(7):72--76, july 1994.
  url: 
  http://www.cs.washington.edu/research/softbots.

9
david filo and jerry yang.
 yahoo home page.
  url: 
  http://www.yahoo.com.

10
james gosling and henry mcgilton.
 the java language environment: a white paper.
  url: 
  http://java.sun.com/whitepaper/javawhitepaper_1.html.

11
ibm, inc.
 infomarket search home page.
  url: 
  http://www.infomkt.ibm.com.

12
ibm, inc.
 query by image content home page.
  url:  http://wwwqbic.almaden.ibm.com/&#126;qbic/qbic.html.

13
martijn koster.
 robots in the web: threat or treat?
  connexions, 9(4), april 1995.

14
lexis-nexis.
 lexis-nexis communication center.
  url: 
  http://www.lexis-nexis.com.

15
michael mauldin.
 lycos home page.
  url: 
  http://lycos.cs.cmu.edu.

16
michael l. mauldin and john r. r. leavitt.
 web agent related research at the center for machine translation.
 in  proceedings of signidr v, mclean, virginia, august 1994.

17
max metral.
 helpful online music recommendation service.
  url: 
  http://rg.media.mit.edu/ringo/ringo.html.

18
nexor.
 cusi (configurable universal search interface).
  url: 
  http://pubweb.nexor.co.uk/public/cusi/cusi.html.

19
university of geneva.
 w3 search engines.
  url: 
  http://cuiwww.unige.ch/meta-index.html.

20
open text, inc.
 open text web index home page.
  url: 
  http://www.opentext.com:8080/omw/f-omw.html.

21
personal library software, inc.
 personal library software home page.
  url: 
  http://www.pls.com.

22
brian pinkerton.
 webcrawler home page.
  url: 
  http://webcrawler.com.

23
brian pinkerton.
 finding what people want: experiences with the webcrawler.
 in  proceedings of the second world wide web conference '94:
  mosaic and the web, chicago il usa, october 1993.

24
brandon plewe.
 the virtual tourist home page.
  url: 
  http://wings.buffalo.edu/world.

25
daniel sears.
 guide to codewarrior magic/mpw.
 development release 1 url: 
  http://www.genmagic.com/magiccapdocs/codewarriormagic/introduction.html, may
  1995.

26
dejanews research service.
 dejanews home page.
  url: 
  http://www.dejanews.com.

27
sun microsystems, inc.
 multithreaded query page.
  url: 
  http://www.sun.com/cgi-bin/show?search/mtquery/index.body.

28
the internet movie database team.
 the internet movie database.
  url: 
  http://www.msstate.edu.

29
the mckinley group, inc.
 magellan: mckinley's internet directory.
  url: 
  http://www.mckinley.com.

30
verity, inc.
 verity home page.
  url: 
  http://www.verity.com.



 about the authors

erik selberg, 
  selberg@cs.washington.edu, http://www.cs.washington.edu/homes/selberg 
department of computer science and engineering  
box 352350  
university of washington  
seattle, wa 98195

erik selberg is pursuing his ph.d. in computer science at the
university of washington. his primary research area involves world
wide web search, although he also has interests regarding system performance
and security as well as multi-agent coordination and
planning. in april, 1995 he created the metacrawler, a parallel web
search meta-service. he graduated from carnegie mellon university in
1993 with a double major in computer science and logic, and received
the first allen newell award for excellence in undergraduate research.

oren etzioni, 
  etzioni@cs.washington.edu, http://www.cs.washington.edu/homes/etzioni 
department of computer science and engineering  
box 352350  
university of washington  
seattle, wa 98195

oren etzioni received his bachelor's degree in computer science from
harvard university in june 1986, and his ph.d. from carnegie mellon
university in january 1991.  he joined the university of washington as
assistant professor of computer science and engineering in february
1991.  in the fall of 1991, he launched the internet softbots project.
in 1993, etzioni received an nsf young investigator award.  in 1995,
etzioni was chosen as one of 5 finalists in the discover awards for
technological innovation in computer software for his work on 
internet softbots.

his research interests include: software agents, machine learning, and
human-computer interaction.


   about this document ... 

 multi-service search and comparison using the 
        metacrawler
this document was generated using the latex2html translator version 95.1 (fri jan 20 1995) copyright &#169; 1993, 1994,  nikos drakos, computer based learning unit, university of leeds.  the command line arguments were: 
latex2html -split 0 www4-final.tex. the translation was initiated by erik selberg on mon oct  9 17:24:12 pdt 1995 

erik selberg 
mon oct  9 17:24:12 pdt 1995



multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler
multi-service search and comparison using the 
        metacrawler