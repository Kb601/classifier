fine-grain parallel cm rivl


fine-grain parallel cm rivl:  a step 
towards real-time multimedia processing


jonathan barber (barber@cs.cornell.edu)
sugata mukhopadhyay (sugata@cs.cornell.edu) 

cs516 final project
professor thorsten von eicken
department of computer science
cornell university





0.0 table of contents

1.0 abstract
2.0 introduction
3.0 rivl and the generic parallel paradigm
3.1 the rivl graph
    3.2 parallelizing rivl
    3.3 continuous media parallel rivl

4.0 implementations
4.1 shared memory implementation
    4.2 networked implementation
    4.3 implementation caveats

5.0 performance results
6.0 extensions and robustness
7.0 conclusions
8.0 references





go back
1.0  abstract 

any form of multimedia processing is typically computationally expensive.  an even 
harder problem is performing some form of multimedia processing on multiple real-time 
continuous streams of data.  in such a paradigm, each frame in a sequence of images 
incurs a very large computational expense.  an obvious yet difficult solution is to divide 
up the problem, and compute the solution in parallel. this paper details the nature of the 
problems and the solutions for dealing with parallel multimedia processing in both shared 
memory and distributed environments.

click here to view a slide-show presentation of this paper.



go back
2.0  introduction:  the evolution of rivl

over the course of the past two years, a large effort has been mounted to develop applications 
that can efficiently and reliably process multimedia data.  the effort manifested itself 
with the construction of rivl (a resolution independent video language).  rivl is a 
multimedia processing package that given a set of images (or a set of a sequence of 
images), can efficiently process these multimedia streams and generate an outgoing image 
(or a sequence of images).  rivl is implemented as a tcl extension that is capable of performing 
common image operations such as overlay, smoothing, clipping, cropping, etc.  
the tcl interface simplifies the process of coding an image processing script.  

recently, rivl has been extended to process continuous streams of multimedia data, and 
generate a corresponding output stream of images.  the extended rivl package, called 
cm rivl, was made possible by treating rivl evaluation as a midpoint in a continuous media object.  this work was facilitated by using cmt (the continuous media toolkit).  

image processing continuous streams of media in real-time is a very hard problem, considering 
today's current state of computer technology.  performing even a simple image oper-
ation over a single sequence of images, and outputting the resultant image[s] in real-time 
requires on the order of a million cpu cycles.  to approach a real-time image-processing 
frame-rate of 30 frames per second, which is the standard frame-rate for perceiving continuous 
motion, would require one of the following items to be true:


to be able to perform image processing operations in less than linear time on a single 
processor
to be able to utilize high-performance technology that does not yet exist
to be able to divide up the work, and perform the image processing in parallel to 
achieve less than linear time performance  


since we have little or no control over the first two items, we have focused our efforts on 
the third.  most image processing routines can be performed in super-linear time if the 
work is divided among an array[s] of parallel processors.  this is true for rivl, and also 
for cm rivl.  

bearing this in mind, we established the project goal to develop an easy-to-use, fast, 
and inexpensive, real-time multimedia processing application.

in section 3.0, we describe a generic method for parallelizing most of the image operations 
in rivl, by exploiting the way that rivl processes an inputted set of images.  in 
section 4.0, we describe two implementations of parallel cm rivl (privl).  the first 
version is designed to run on shared memory machines.  the second version is designed to 
run over a cluster of workstations.  in section 5.0, we present an analysis of performance 
results.  in section 6.0, we describe some improvements to our implementations.   finally, 
in section 7.0, we draw some conclusions and analyze our progress.  



go back
3.0  rivl and the generic parallel paradigm


go back
3.1  the rivl graph

we begin our discussion of rivl by introducing the rivl evaluation graph.  



in order for rivl to execute, it requires a set of multimedia input data, and a control 
rivl script.  the rivl script is a sequence of tcl-rivl commands that specify what image 
processing operations should occur on the input data.  once rivl is invoked, the rivl 
script is translated into the rivl graph, as pictured above.  each node corresponds to 
some image operator (e.g. im_smooth, im_canny, etc.), and each edge or signal 
corresponds to the actual image data.  those nodes lying inside of the illustrated rectangle 
above correspond to true image operators.  those nodes lying outside of the rectangle are 
the rivl i/o nodes.  the nodes outside and to the left of the rectangle correspond to read 
nodes (i.e. one read/node per image [or stream]), and the node to right of the rectangle 
corresponds to the write node.     

we want to emphasize that construction of the rivl graph does not compute on any 
multimedia data.  the rivl graph is merely the control-flow structure through which 
each inputted sequence of data must propagate to generate the outputted, processed image.  

there are two phases in processing data using the rivl graph once it has been constructed.  
the first phase manifests itself in a graph traversal from right-to-left.  this is 
what makes rivl an efficient image processing mechanism.  the first node that is evaluated 
is the write node (the right-most node).  by traversing the graph in reverse-order, 
rivl decides at each node exactly how much data the output signal requires from the 
input signal.  the evaluation is reverse-propagated from the write node, through the graph, 
and back to every read node.  once the reverse-propagation completes, every node in the 
graph knows exactly how much data from each input signal is required to compute the 
node's corresponding output signal.  the multimedia data is then processed on the second 
traversal, which conforms to a left-to-right traversal of the rivl graph, propagating the 
input data forwards through the graph, only operating on data that is relevant to the final 
output image. 


go back
3.2  parallelizing rivl

we can summarize the preceding section into the statement that, the amount of data that is 
fetched from each read node is exactly a function of the output of the 
write node.  combining 
this notion with the fact that most of the image processing operations in rivl do 
not create dependencies from one pixel to another in a given input image, we can derive a 
simple for mechanism for "dividing up the work", and parallelizing rivl.  

instead of running rivl on a single processor, we spawn multiple rivl processes on different 
processors, and have each process work towards computing a different segment of 
the output data.  we define the notion of a single master rivl process, and multiple 
slave rivl processes.  each slave process is started on a different processor.  once 
started, the slave process sits idle, listening for instructions from a master process.  after 
the slave processes have been started, a master process is created.  the master process 
determines how many slaves are "available for work".  once a control connection is established 
between the master and every slave, the master assigns each slave a logical id# 
(the master id# is 0, the slave's id# ranges from 1 to n slaves).  after each slave is 
assigned an id#, the master sends each slave the total number of processes "available for 
work", followed by a copy of the rivl script.  once each slave (and the master) receives 
the rivl script, they each generate a copy of the rivl graph, and perform the right-to-
left traversal independently.

the difference between the right-to-left traversal now, is that the logical id# for the current 
processor and the total number of processes becomes a factor in determining how 
much computation gets done for each process.



according the figure above, the amount of data fetched from each read node is no longer a 
function of the output of the write node, but is now a function of:

the process's logical id#
the total number of processes
and, is a function of the write node's output


that is, each rivl process  is responsible for computing a different, independent portion 
of the final output data, which is based on the above parameters.  hence the term "fine-grain 
parallel cm rivl".  our approach is fine-grained in that each rivl process performs 
the same set of computations, on different data.

actual data computation (the left-to-right graph traversal) occurs when the master says 
"go".  each slave and the master process computes their appropriated portion of the output 
image.  


go back
3.3  continuous media parallel rivl



the model of parallelization for rivl just described maps smoothly to cm rivl.  with 
cm rivl, there is an initial setup phase for each slave process and the master process, as 
previously described  (the master process sends each slave its logical id#, the total number 
of processes, and a copy of the rivl script.  each rivl process then computes the 
rivl graph and makes the  right-to-left traversal).  

the image processing for computing each output frame in a continuous media stream 
occurs as follows:

there is a cmo (continuous media object), which captures and manages continuous 
streams of data, and resides as part of the master process.  

when the cmo has captured all of its input data for a single output image, it contacts 
the master's parallel synchronization device, and tells each rivl process (slaves and 
the master) that data is ready to be fetched, and that computation can begin asap.
  
each rivl process then fetches only the input data it needs to generate its segment of 
the output data, and makes the left-to-right traversal through the graph.
  
the output data from each rivl process is then written back to a buffer within the  
cmo, where the data is re-assembled into a single data-output object.
  
each rivl process then blocks, listening for further instructions from the cmo as to 
when another image will be ready for processing.  


using this method, for a given stream of multimedia data, the construction of the rivl 
graph and a reverse-traversal of the graph are performed only once at setup-time.  the 
actual image processing only requires one traversal of the graph on each rivl process, 
where the computation area is distributed among all of the rivl processes.



go back
4.0   implementations

based on the generic parallelization scheme described in the preceding section, we have 
developed two implementations of parallel cm rivl.  each implementation has its own 
synchronization mechanism for parallelizing the independent rivl processes, and its own 
mechanism for transferring data.  


go back
4.1  shared memory implementation



the shared-memory implementation is illustrated above.  each rivl process resides on a 
different processor, but each processor resides on the same machine, which has access to 
the same shared memory segment.  

this implementation mirrors the generic parallel model  described in section 3.

implementation details:
   
the initial setup is facilitated by using tcp-ip multi-cast via tcl-dp.

the process synchronization is facilitated using unix semaphores.

the data transfer is facilitated using shared-memory reads and writes via unix-ipc.

the program was compiled for a sparcstation running sunos.


this model operates as follows:  

following the initial setup phase, the cmo works at capturing all data necessary to compute 
a single rivl output frame.  once the cmo captures all the necessary data, it tells 
each rivl process to begin processing by means of an entry semaphore.  each rivl process 
then reads only the data relevant to its own output via a shared-memory read.  once 
the left-to-right evaluation of the rivl graph completes, the rivl process then performs 
a shared-memory write to the memory region containing the output image that is accessible 
by the cmo.  the rivl process then blocks at an exit semaphore until all of the 
rivl processes complete computation for the same frame of data.  once every rivl process 
blocks, the master rivl process un-sets the exit semaphore, and each rivl process 
waits again at the entry semaphore, until the cmo again releases it.


go back
4.2  networked implementation



the networked implementation is illustrated above.  each rivl process resides on a different 
processor, and each processor resides on a different machine.  

this implementation also mirrors the generic parallel model described in section 3.

implementation details:

the initial setup is again facilitated by using tcp-ip multi-cast via tcl-dp.

the data transfer is facilitated using active-messages over u-net.   

the synchronization mechanism is implicit via the active-messages paradigm.

the program was compiled for a sparcstation running sunos.


this model operates as follows:

like its shared-memory counterpart, this model performs the initial setup using ip multicast 
to establish the active message connections from the master to each slave rivl process.  
the cmo works at capturing all data necessary to compute a single rivl output 
frame.  this model differs from the generic-model in that the master process knows 
exactly what portion of the input data each rivl process needs to evaluate their rivl 
graph.  once the cmo captures all the necessary data, it tells each rivl process to begin 
processing by issuing a gam_store() to each rivl process.  once the message is received 
by each rivl process, a handler is invoked which tells the rivl process that it can begin 
evaluating its rivl graph on the transferred data.  once the output data is computed, the 
rivl process then issues a gam_store() to the master process, specifying exactly where 
the sent data should be stored in the final output image buffer managed by the cmo.  
eventually, a handler routine in the master process will update a "received-from list".   
once the master receives data from each rivl process, the cmo outputs the computed 
frame, and begins processing the next multimedia frame.

the process synchronization mechanism is implicit with the actual data-transfer, in that, a 
rivl process cannot begin evaluating its graph on a given frame segment, until it receives 
an active-message from the master process.  similarly, the master process cannot move 
on to the next multimedia image until it receives an active-message from each slave process.

another subtle point is that by having the master determine how much of the input data 
each rivl process requires, rather than having the rivl process itself determine this 
information, we reduce the round-trip communication rate from master to slave.  having 
each rivl process compute its own region, would require a gam_request(), followed by 
a gam_reply() by the master process.  instead, the master decides how much data each 
rivl process needs and simply issues a single gam_store().  


go back
4.3  implementation caveats

our actual executables are not spmd.  there is a separate executable for the master process, 
and another executable for each slave process.  this didn't cause any problems when 
developing the shared-memory implementation.  however, since active-messages ver 1.1 
assumes a spmd model, we ran into problems when specifying am handlers in both the 
master process and the slave processes.  

when the master process received active-messages from any slave process, the slave process 
attempted to invoke an am handler in the master that existed in the slave, but not in 
the handler.  the situation was the same when a slave process received an active message 
from the master.  

we overcame this shortcoming in  by modifying the active-message's source code.  the 
modification allows an application  to register a handler with active-messages by calling
 
hid uam_reg_handler(handler_t handler)  

"handler_t handler" corresponds to the handler's 
virtual address.  the process returns an "hid", which is an integer, but stands for 
"handler id#".   in our implementation, since only the master executable and slave executable 
are different, the master and each slave must register their handlers with the active-
message's library.  now, when a process sends an active message (from slave to master 
and vice versa), it no longer ships the processes's virtual address of the handler, but rather, 
ships a logical id#, corresponding to the handler to be invoked.  the active-message's 
library maintains a look-up table that is indexed by the logical id#.  the logical id# corresponds 
to a process handler's virtual address, which is then invoked from active-messages. 



go back
5.0  performance results

we ran our shared-memory experiments on a quad-processor sparcstation 10 running sunos.  
our networked implementation was tested by using 4 atm-connected sparcstation 20s running sunos.    

we constructed two different test cases, named test 1 and test 2.  the two 
tests perform the following image operations:


test 1:there are 2 input sequences of images.  the first image sequence (im1) 
is scaled, rotated, and copied four times.  the resulting output is then overlayed onto 
the second image sequence (im2), and then output.

test 2:there are 2 input sequences of images.  im1 is scaled, rotated, and copied
four times.  im2 is smoothed.  the output from im1 is then overlayed ontop of the output for
im2.


overall, test 2 is a more computationally expensive set of operations than test 1.  
this fact is illustrated by our experimental results.



from our graphed results above, the shared-memory implementation performs
somewhat better than our networked implementation.  both implementations, however, perform better 
than their serial counterparts (the green bar graph). one observation was that the networked
implementation exhibited a large spread of timings for different frames, and this we attributed
to our process getting preempted. the behavior was not visible on the shared memory implementation
as our process was sleeping, waiting for the semaphores to change, while the process in the network
implementation busy-waits. hopefully, an interrupt driven implementation of active messages
would cure this.

note:  in all tests, the processor speed is relatively equal.

results:  

shared memory:  in both tests 1 and 2, the performance gains exhibited the following
patterns:

from 1 to 2 processors:  performance is nearly doubled.
from 2 to 3 processors:  again, our performance is nearly doubled.
from 3 to 4 processors:  the performance increase is negligble.  
performance is not increasing either because the communication overhead exceeds 
the performance gain, or because the processors are un-optimally load-balanced (probably the latter).


networked implementation: 

from 1 to 2 processors:  performance is nearly doubled.
from 2 to 3 processors:  there is a small improvement in performance, however, the shared 
memory implementation appears to do a little better.
from 3 to 4 processors:  the performance increase is again negligble.  the explanation for 
this is probably the same as in the shared memory experiment.
 



go back
6.0  extensions & robustness

there are a number of improvements that can and should be made to improve overall 
performance and robustness of our parallelization scheme.


improve the load-balance:  the largest improvement involves improving the 
load-balance among all of the rivl-processes by using a "hungry puppy strategy" for 
dividing up the work.  our current implementations statically allocate work to each rivl 
process.  the location and the amount of data that is needed for each rivl process is 
determined as a function of the number of processes and the process id.  as indicated 
from our experimental results, there is no significant boost from 3 to 4 rivl processes 
using our shared-memory implementation.  we can partly attribute this problem to an 
un-optimal load balance.  

modifying the networked implementation should prove more trouble-some, and while 
improving the overall load-balance, will probably increase the communication overhead, 
as more active-message will have to be sent and processed.

modifying the shared-memory version should be easier.  the current synchronization 
mechanism is implemented by using unix semaphores.  no rivl process is allowed to 
begin executing the next frame until all rivl processes have completed execution of the 
current frame.  the output-image is currently divided up by the number processes available 
for work.  we could improve the load-balance for this implementation by doing two 
things:  (1)  by dividing up the output-image work regions into more numerous smaller 
segments; and (2) for a current frame, allow rivl processes to complete executing their 
output segment, and grab another segment from the still-need-to-be-computed queue 
residing on the master process.  this implementation will improve load-balance by allowing 
less-busy processes to contribute equally to the entire output image, while giving 
busier processors the time they need to compute their data without becoming a bottleneck 
for the entire output image.

improve reliability and fault-tolerance:  in real-time systems, it is not uncommon 
for things to go wrong.  specifically, what should happen in the even that a slave rivl 
process crashes?  our current implementations do not account for such mishaps.  if a process 
were to malfunction, due to either hardware or communication failure, our implementation 
would fail.  

port our atm-sparc implementations over to fast ethernet pc:  in designing any 
system, cost is always an issue.  the purpose for implementing privl over active-messages 
was to utilize the lower cost of workstations and networks as compared to expensive 
parallel machines.  the cost of higher performance pcs is rapidly on the decline.  adapting 
our implementations to fast ethernet is a natural step in reducing the cost of high-performance 
cm privl.  the actual transition from atm-sparc to fast ethernet-pc is 
merely a matter of getting active-messages to work over fast ethernet.
  



go back
7.0  conclusions

we were looking for significant speedups in parallel cm rivl as we moved from 1 to n 
processors (n being no more than 4).  our results are definitely encouraging.  in both our 
shared-memory implementation and our networked implementation, we obtained good speedups
up to four processors. in order to process real-time data, we need to approach a frame-processing
rate of close to 30 frames per second, or rougly 33 ms per frame.  for the operations we have tested,
we will require upwards of 30 similar processors to achieve the desired frame rate.  

we do not have results for more than four processors. however, by examining our results, we can
determine that under the current implementations, the processes running parallel cm rivl
will not be load-balanced.  

unfortunately, we must conclude that our implemenations as they stand will not scale 
to upwards of 30 processors to achieve the desired frame rate.  however, further work is under 
way to address this load-balancing problem.  furthermore, a "hungry-puppy" 
object-tracking algorithm is currently being incorporated into privl.  
the experimental results from this should be available shortly.

we have however made significant progress in parallelizing cm rivl.  cm rivl is a non-trivial 
application, and our parallelization scheme works for most of the standard rivl image operations.
 
   


go back
8.0  references

 jonathan swartz, brian c.  smith
    
    a resolution independent video language 
    proc. of the third acm international conference on multimedia, san
    francisco, ca, november 5-9, 1995. 

 lawrence a. rowe, brian c. smith,
	continuous media player,
	third international workshop on network and operating systems support
	for digital audio and video, nov. 12-13, 1992, san diego, ca.

 brian c. smith, lawrence a. rowe, stephen c. yen
	
 	tcl distributed programming,
	proc. of the 1993 tcl/tk workshop, berkeley, ca, june 1993.

 von eicken, t., d. e. culler, s. c. goldstein, and k. e. schauser,
	
	active messages: a mechanism for integrated communication and
	computation.
	proceedings of the 19th int'l symp. on computer architecture,
	may 1992, gold coast, australia.

 anindya basu, vineet buch, werner vogels, thorsten von eicken,
	
	u-net: a user-level network interface
	for parallel and distributed computing,
	proc. of the 15th acm symposium on operating systems principles,
	copper mountain, colorado, december 3-6, 1995.

 sugata mukhopadhyay, arun verma, 
	
     cmrivl - a programmable video gateway,
	cornell university, spring '96 





 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl
 fine-grain parallel cm rivl