jpeg encoding using perceptual quality  

multimedia systemsjpeg encoding using perceptual quality  
nobuhiko mukai (mukai@cs.cornell.edu) 
lucy y. wu(yuwu@cs.cornell.edu) 
mikio sakurai (msakurai@sunlab.cit.cornell.edu)
 


abstract:

the key point of image compression is to achieve a low bit rate in the 
digital representation of an input image or video signal with minimum 
perceived loss of picture quality. over the past several years there 
have been many attempts to incorporate perceptual masking models into 
image compression system. based on the "pre-quantization", we 
developed two new models. compared to jpeg default setting, both of 
our models significantly lower the bit-rate and keep the almost same
image quality.



1. introduction

the key point of image compression is to achieve a low bit rate in 
the digital representation of an input image or video signal with 
minimum perceived loss of picture quality. since the ultimate 
criterion of quality is judged or measured by the human receiver, 
it is important that the compression(or coding) algorithm minimizes 
perceptually meaningful measures of signal distortion.

jpeg default quantization tables(qt) are based on psychovisual thresholding and
are derived empirically. but because only one qt is available for every image,
the default qt is image independent and can not be used to achieve the optimized
compression result for each specific image.

some perceptual models have been developed to calculate the image dependent qt. but
because every block in the image contributes different properties to the total image,
one qt that is best for the whole image is not always best for every block. if
we can quantize every block using different qt specifically suitable for that
block, we can get the most optimized compression result for every block.

because jpeg allows only one qt for each image, pre-quantization is proposed by
johnston and safranek(j&s)[1]. for every block, one specific masking threshold is
calculated and used to zero out the perceptual irrelevant coefficients while the
other coefficients passed unchanged. then one base qt will be finally used to
quantize for all the remaining coefficients in each block.

despite the benefits of simple implementation, the j&s model has the disadvantage of
computing a single masking elevation for each input block. this means that there
is no information about the distribution of energy within the block.this
problem can be overcome by applying the contrast masking model for each dct
coefficient(model-1).

the computation for model-1 is somewhat complex. this led us to design a second
one(model-2) based on the luminance ratio of block to total image to replace the j&s
model. this second model has the same single masking elevation problem, but the calculation
is more simple.

our test result shows, compared to the jpeg default setting, both of our models
reduced the bit rate 10% with little or no perceived loss in quality.

the rest of this paper is organized as follows. section 2 describes algorithms to
"prequantize" a jpeg image. section 3 describes two perceptual models designed by us. section 4 describes the detailed evaluation of our models, and
section 5 reviews related work and future extensions.


2. prequantization
2.1 quantization 
the baseline jpeg encoder consists of three major components, a forward dct,
quantization, and entropy coding. the step of the quantization is to take the raw
output of the dct and quantize the coefficients by dividing it, coefficient by
coefficient, by qt, and rounding to the nearest integer. the purpose of quantization
is to achieve compression by representing dct coefficients with no greater precision
than is necessary to achieve the desired image quality. in other words , the goal of
this processing step is to discard information which is not visually significant.

2.2 perceptual model
many studies have attempted to derive a computational model of this visual masking
level. for each block in the input image, the model attempts to determine to what
degree the features present in that block inhibit the visual system from the
distortion introduced by the compression/decompression process. from these points, 
it is possible to determine a masking threshold for each dct coefficient.

2.3 j&s's model 
johnston and safranek have developed a framework for computing a locally adaptive
masking model based on an engineering framework. they assume that the total masking
level for any block of the input can be represented as a base masking level, and
other multiplicative elevation factors that represent the contribution of input
dependent properties of the visual system to the total mask. this model may be
expressed as:

m(u,v) = global(u, v) x local(u, v)   --- (1)

where m(u, v) is the masking level for frequency (u, v) of the input block,
global(u,v) is the base masking level which depends only on global properties, and
local(u, v) handles the image specific local variation in the masking threshold. the
adaptation is derived as a function of the block standard deviation using the
following formula:

 ---(2)
figure 1. j&s's model

this applies to all of the ac coefficients. the masking elevation for the dc
coefficient is always set to unity. this model has the advantage of simple
implementation, and works well in practice. it has the disadvantage of computing a
single masking elevation for each input block.

figure 2 illustrates the structure of such an encoder. the forward transformation is identical to the one in baseline jpeg. at this point, the dct coefficients are 
input to the perceptual model which generates the data dependent quantization table for that block. this table and the raw dct coefficients are now input to a
"pre-quantizer". the purpose of this module is to zero out the coefficients that
have a magnitude less than the corresponding entry in the quantization table for
that block, and pass the other coefficients through unchanged. finally these
prequantized coefficients are quantized and entropy coded as in standard
jpeg. 

in the dequantization step, only the base qt is used.

figure 2. a structure of an encoder with prequantization


3. our models

our models intends to take advantage of the above prequantization method.

3.1 model-1 the j&s's model uses the perceptual model but does not
consider the distribution of the energy within the block when calculating the
block-specific masking threshold. to overcome this problem, we employ a visual
masking that has been widely used in vision models, based on work by legge and
foley[2]. given a dct coefficient c(u,v) and a luminance threshold t(u,v), the
masked threshold m(u,v) is 

m(u,v)= max[t(u,v), |c(u,v)|^w(u,v) *t(u,v)^(1-w(u,v))] ---(3)

w is a constant that lies between 0 and 1. when w=0, no masking occurs, 
and when w=1, we have "weber's law" behavior. for our experiment, an 
empirical value of w =0.7 was used. 

in our implementation to calculate the masking threshold, we did not
 calculate the luminance threshold by using peterson's model[3] as 
suggested by watson[4]. in addition, as indicated in jpeg standard, 
jpeg default quantization tables are based on psychovisual thresholding 
and are derived empirically. if it is divided by 2, the almost 
indistinguishable image can be reconstructed. that means the default qt 
can be treated as a general luminance threshold. so, we replaced the 
t(u, v) in equation(3) by "jpeg default qt value" / 2.

as a global masking level, we used default jpeg qt.

3.2 model-2 
the masking threshold computation for model-1 is rather complex. this 
led us to design the second model(model-2) based on the luminance ratio 
of block to total image. this model has the same single masking elevation 
problem, but the calculation is more simple.

basically, model-2 follows the j&s masking model (equation 1). but for 
the calculation of multiplicative elevation factors (local(u,v)), we 
use the luminance ratio of each block to the total image.

well known "weber's law" is expressed as:

df / f = constant (0.02)  ---(4)

f is luminance and df is the just noticeable difference.

this equation means our perception is sensitive to luminance contrast 
rather than the absolute luminance values themselves. at a given luminance 
f, if the block's luminance is only a little bit differ from luminance f, 
then the block is less visible and we can drop a lot of perceptually 
unimportant information.

the adaptation is derived as a function of the ratio of the block's average 
luminance to the luminance average of the total image using the following 
formula:

 ---(5)
figure 3. model 2: masking model based on luminance ratio

maximum threshold elevation "max_e" and minimum threshold (minimum 
luminance ratio) "min_t" are the parameters we need to experiment for. 

as a global masking level, we used default jpeg qt.


4. image quality and bitrate
we compared the bitrate(bits/pixel) and image quality of our model-1 
and model-2 with those of the baseline jpeg. for model-2, first we 
need to optimize the parameter. we worked on several images and found that
these two parameters(max_e, min_t) are not strongly sensitive to snr.
to the different value of luminance ratio parameter min_t, snr is 
almost constant.threshold elevation parameter max_e has weak relation with 
snr and if it gets bigger, snr becomes worse. this feature is common 
to all images. table 1 shows a result we got on image "lena".


table 1: parameter(max_e,min_t) dependence of snr 

for the following experiment, we choose the preferable value max_e = 2 and 
min_t = 0.01 for each parameter of model-2. 
five images were used in this experiment: a photo of human 
face("lena"), a flower scene("flowers"), a photo of animal face("baboon"), 
two photo of airplane(f-18 and pitts).

bitrate is calculated after compressing the image file using the pack
(huffman coding) program. for the evaluation of the image quality, we 
used two evaluation method. the first is snr(signal to noise ratio). in 
order to provide further insight into the subjective quality of the 
models, we used dcon metric[5]. this algorithm takes as input two images, 
a reference and a test, compare the difference of luminance pixel by pixel. 
the formula is as follows:

dcon = 1/n * sum((y1-y2)/(y1+y2) + 23) --- (6)

this method is simple but very competitive with other complicated human eye
model metric. 

table 2 summarize the results of these experiments.


table 2: image quality(snr,dcon) vs bitrate 

both our model-1 and model-2 compress 10% better than the baseline jpeg
with almost no perceptual loss in quality. figure 4 shows one example 
("flowers") of output image of our models in comparison with original 
image and that of baseline jpeg.
(1)
(2)
(3)
(4)
(1) original image (2) baseline jpeg (3) model 1 (4) model 2 
figure 4. a sample image comparison

5. related work and conclusions 
there have been many attempts to incorporate perceptual masking model into 
image compression systems[6,7]. johnston-safranek model[1] and watson[4] 
model are perhaps the most well known perceptual model. j&s model has 
investigated locally optimized prequantization table. watson model has 
investigated image dependent masking model which incorporates not only 
global conditions but also accounts for local contrast masking.

klein of u.c.berkery optometry school has reported techniques for 
improving the quality of jpeg with high compression rate in viewpoint 
of vision community[8]. he suggests that with improved human vision 
models the quantization step could be made more effective by 
considering effects such as mean luminance, color, bandpass filters 
in spatio-temporal frequency and orientation, contrast masking and 
human contrast sensitivity.

the primary contribution of our work is that it details encoding-specific
prequantization algorithms that compress images at high compression rate 
with minimal artifact. our future work will include extending this work to 
include other human eye related factors such as spatio-temporal frequency 
and orientation.



 references 
[1]johnston, j. d. and safranek, r. j., "a perceptually tuned sub-band 
image coder with image dependent quantization and post-quantization data 
compression." proc. icassp 89, glasgow, scotland, vol.ma, may 1989, pp. 
1945-1948

[2]g.e.legge and j.m.foley, "contrast masking in human vision", journal 
of the optical society of america. 70(12), 1458-1471(1980).

[3]a.j.ahumada and h.a.peterson, "luminance-model-based dct quantization 
for color image compression", spie:human vision, visual processing, and 
digital display iii, vol.1666,1992, pp365 - 374.

[4]watson,a.b., "dct quantization matrices visually optimized for 
individual images", spie:human vision, visual processing, and digital 
display iv, vol 1913 feb.1993, pp202 - 216.

[5]daniel r. fuhrmann, john a. baro, and jerome r. cox
experimental evaluation of psychophysical distortion metrics for 
jpeg-encoded images, spie:human vision, visual processing, and 
digital display, vol.1913, pp179 - 190.

[6]s.j.daly, "the visible difference predictor: an algorithm for the 
assessment of image fidelity",spie:human vision, visual processing, and 
digital display iii, vol.1666,1992, pp2 - 15.

[7]j.l.mannos and d.j.sakrison, "the effects of a visual fidelity 
criterion on the encoding of images", ieee transaction on information 
theory it-20, pp525 - 536

[8]stanley a. klein, amnon d. silverstein and thom carney, "relevance of 
human vision to jpeg-dct compression", spie:human vision, visual 
processing, and digital display iii, vol.1666,1992, pp200 - 215.

 
postscript file
file last modified -  4 december 1995  

 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality
 jpeg encoding using perceptual quality