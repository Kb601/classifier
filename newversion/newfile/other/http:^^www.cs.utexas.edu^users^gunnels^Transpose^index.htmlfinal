transpose case
 
john a. gunnels
     
         department of computer science 
            university of texas at austin 
            gunnels@cs.utexas.edu 
        
       
markus dale
     
         department of computer science 
            university of texas at austin 
            mdale@cs.utexas.edu 
        
       
notes : in a lesson on not putting off this stuff (waiting for llano or
eureka to become available) i ran into a weird bug on sunday (well, since we
really started on sunday i guess that is kind of redundant).  for some reason 
the permutation routine "chokes" when the pieces being sent around climb above
a certain size.  for example -- i can do a 1600 sq. mult on 16 processors
with a block size of 10 but not 20.  i checked for malloc failures but that
wasn't it(*).  so my results here are very sketchy.  actually, i would rather
get some timings on eureka or llano before i try to tune this thing.

(*)well, there is no problem on eureka in this respect.  some preliminary
data is also more encouraging as far as performance on eureka.  although it
is not what we are aiming for, a 4x4 grid on eureka achieves 35.xx mflops 
when the local size hits 400x400 or so.  
(**)the problem has been fixed -- however, it was a matter of using irecv instead
of receive (basically) in implementing the collect (perhaps i should have just used 
mpi_allgather).  however, these messages were not very big -- i would need to read 
about the sp2 architecture i guess but this seems pretty fragile to me.
btw -- the code does handle non-square matrices and meshes, i just need to
collect timings for those cases.




note : i have tweaked the code a little.  basically, add 2 mflops to
everything here.  i will post the chart soon (3/27/96).
 the code enclosed does not perform accuracy testing -- it was removed to 
make the runs because it creates the global matrix on each processor.  we do
have a version (on both spice and eureka) that does the testing.
main.c
csmmult1.c
colrow1.c
globals.h
rand.c

note : there are at least 4 simple things we could do to improve the performance
of this code. 
1. instead of scattering followed by permuting we could simply permute (from
1-to-many).  that is send the blocks immediately to the processor that they
will arrive at after both the scatter and the permute step.  to really make
the code unreadable, we believe that you could use non-blocking sends to overlap
the copying to the send buffer with the sending of blocks. (1 hour to recode and
test)
2. using mpi_allgather instead of our hand-written bucket-collect.  (30 minutes
to re-code and test).
3. there is a simple test to see if you are sending to yourself for all of these
routines. this might improve performance a great deal on grids that are far
from sqaure (although, this does partially void #2 -- or perhaps not, if mpi is
"smart" enough to figure this out itself).  the only "drawback" would be that it really wouldn't be implementing the same code on the 1x1 mesh and on a small machine like eureka this might make the scalability appear either "bad" or hard to figure an alpha, beta, gamma equation for. 
4. on a square mesh there is a very simple trick that would really speed things
up (i think).   in the scatter step of a's rows simply send to the same row
number that you are the col number of (that is if you are in column 0 you send
to row 0 within column 0, 1->1 etc.).  then perform a broadcast within columns.
then you do the analogous thing for the columns of b.  i am pretty sure that prof. van de geijn discussed this in class as one of the shortcuts.  i really mention it just to point out that that is not what we did (because we are presenting data for the square mesh case).







transpose case
transpose case
transpose case
transpose case
transpose case
transpose case
transpose case
transpose case
transpose case
transpose case